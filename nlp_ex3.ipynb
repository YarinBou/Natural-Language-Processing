{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3- Yarin and David - NER",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-WJBimYDLJS"
      },
      "source": [
        "# Assignment 3\n",
        "Training a neural named entity recognition (NER) tagger "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3enPCGBF8FlX"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5QSIEoyDdWh"
      },
      "source": [
        "In this assignment you are required to build a full training and testing pipeline for a neural sequentail tagger for named entities, using LSTM.\n",
        "\n",
        "The dataset that you will be working on is called ReCoNLL 2003, which is a corrected version of the CoNLL 2003 dataset: https://www.clips.uantwerpen.be/conll2003/ner/\n",
        "\n",
        "[Train data](https://drive.google.com/file/d/1hG66e_OoezzeVKho1w7ysyAx4yp0ShDz/view?usp=sharing)\n",
        "\n",
        "[Dev data](https://drive.google.com/file/d/1EAF-VygYowU1XknZhvzMi2CID65I127L/view?usp=sharing)\n",
        "\n",
        "[Test data](https://drive.google.com/file/d/16gug5wWnf06JdcBXQbcICOZGZypgr4Iu/view?usp=sharing)\n",
        "\n",
        "As you can see, the annotated texts are labeled according to the IOB annotation scheme, for 3 entity types: Person, Organization, Location."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf1AQvsoTmn-",
        "outputId": "4945d667-0239-41cb-f1dd-b3257c9ad771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ul2Y3vuPoV8"
      },
      "source": [
        "**Task 1:** Write a funtion for reading the data from a single file (of the ones that are provided above). The function recieves a filepath and then it encodes every sentence individually using a pair of lists, one list contains the words and one list contains the tags. Each list pair will be added to a general list (data), which will be returned back from the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prgzgtt8Jw4Y"
      },
      "source": [
        "def read_data(filepath):\n",
        "    # initialize a general list of list pairs ([word], [tag])\n",
        "    data = []\n",
        "    # open properly the filepath\n",
        "    with open(filepath) as file_:\n",
        "      # a list of words\n",
        "      words = []\n",
        "      # a list of tags\n",
        "      tags = []\n",
        "      # read the data from a single file\n",
        "      for line in file_:\n",
        "        # check if a line is empty\n",
        "        if line == '\\n':\n",
        "          data.append((words, tags))\n",
        "          words = []\n",
        "          tags = []\n",
        "        # if a line contain a pair of word and tag\n",
        "        else:\n",
        "          word, tag = line.split()\n",
        "          words.append(word)\n",
        "          tags.append(tag)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train = read_data('https://drive.google.com/file/d/1hG66e_OoezzeVKho1w7ysyAx4yp0ShDz/view?usp=sharing')\n",
        "# dev = read_data('https://drive.google.com/file/d/1EAF-VygYowU1XknZhvzMi2CID65I127L/view?usp=sharing')\n",
        "# test = read_data('https://drive.google.com/file/d/16gug5wWnf06JdcBXQbcICOZGZypgr4Iu/view?usp=sharing')\n",
        "train = read_data('/content/drive/MyDrive/connl03_train.txt')\n",
        "test = read_data('/content/drive/MyDrive/connl03_test.txt')\n",
        "dev = read_data('/content/drive/MyDrive/connl03_dev.txt')"
      ],
      "metadata": {
        "id": "oRuhP4yeepJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train)"
      ],
      "metadata": {
        "id": "kZEyxzmhYG4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuGwk6OwRWGS"
      },
      "source": [
        "The following Vocab class can be served as a dictionary that maps words and tags into Ids. The UNK_TOKEN should be used for words that are not part of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rKIB5o_vQO8"
      },
      "source": [
        "UNK_TOKEN = 0\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.word2id = {\"__unk__\": UNK_TOKEN}\n",
        "        self.id2word = {UNK_TOKEN: \"__unk__\"}\n",
        "        self.n_words = 1\n",
        "        \n",
        "        self.tag2id = {\"O\":0, \"B-PER\":1, \"I-PER\": 2, \"B-LOC\": 3, \"I-LOC\": 4, \"B-ORG\": 5, \"I-ORG\": 6}\n",
        "        self.id2tag = {0:\"O\", 1:\"B-PER\", 2:\"I-PER\", 3:\"B-LOC\", 4:\"I-LOC\", 5:\"B-ORG\", 6:\"I-ORG\"}\n",
        "        \n",
        "    def index_words(self, words):\n",
        "      word_indexes = [self.index_word(w) for w in words]\n",
        "      return word_indexes\n",
        "\n",
        "    def index_tags(self, tags):\n",
        "      tag_indexes = [self.tag2id[t] for t in tags]\n",
        "      return tag_indexes\n",
        "    \n",
        "    def index_word(self, w):\n",
        "        if w not in self.word2id:\n",
        "            self.word2id[w] = self.n_words\n",
        "            self.id2word[self.n_words] = w\n",
        "            self.n_words += 1\n",
        "        return self.word2id[w]\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDKYryfKfNdh"
      },
      "source": [
        "**Task 2:** Write a function prepare_data that takes one of the [train, dev, test] and the Vocab instance, for converting each pair of (words,tags) to a pair of indexes. Each pair should be added to data_sequences, which will be returned back from the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noIY3zWKvhBd"
      },
      "source": [
        "vocab = Vocab()\n",
        "\n",
        "def prepare_data(data, vocab):\n",
        "    # initialize a return value\n",
        "    data_sequences = []\n",
        "    # loop over a list of list pairs ([word], [tag])\n",
        "    for words, tags in data:\n",
        "      # convert each pair to a pair of indexes\n",
        "      data_sequences.append((vocab.index_words(words), vocab.index_tags(tags)))\n",
        "\n",
        "    return data_sequences, vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences, vocab = prepare_data(train, vocab)\n",
        "dev_sequences, vocab = prepare_data(dev, vocab)\n",
        "test_sequences, vocab = prepare_data(test, vocab)"
      ],
      "metadata": {
        "id": "P4GM3aeZhw5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_sequences, vocab)"
      ],
      "metadata": {
        "id": "Xq_wH8J5iPHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UccfiRRtiEet"
      },
      "source": [
        "**Task 3:** Write NERNet, a PyTorch Module for labeling words with NER tags. \n",
        "\n",
        "*input_size:* the size of the vocabulary\n",
        "\n",
        "*embedding_size:* the size of the embeddings\n",
        "\n",
        "*hidden_size:* the LSTM hidden size\n",
        "\n",
        "*output_size:* the number tags we are predicting for\n",
        "\n",
        "*n_layers:* the number of layers we want to use in LSTM\n",
        "\n",
        "*directions:* could 1 or 2, indicating unidirectional or bidirectional LSTM, respectively\n",
        "\n",
        "The input for your forward function should be a single sentence tensor.\n",
        "\n",
        "*note:* the embeddings in this section are learned embedding. That means that you don't need to use pretrained embedding like the one used in class. You will use them in part 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke1LyUQNyQaM"
      },
      "source": [
        "class NERNet(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, n_layers, directions):\n",
        "        super(NERNet, self).__init__()\n",
        "        self.direction = False if directions == 1 else True\n",
        "        self.input_features = hidden_size * 2 if self.direction else hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, bidirectional=self.direction)\n",
        "        self.out = nn.Linear(self.input_features, output_size)\n",
        "\n",
        "    \n",
        "    def forward(self, input_sentence):\n",
        "        # TODO: your code...\n",
        "        dim = len(input_sentence)\n",
        "        hidden = None\n",
        "        embedded = self.embedding(input_sentence)\n",
        "        lstm_output, _ = self.lstm(embedded.view(dim, 1, -1), hidden)\n",
        "        output = self.out(lstm_output.view(dim, -1))\n",
        "        return output\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEGSQdeUkTP8"
      },
      "source": [
        "**Task 4:** write a training loop, which takes a model (instance of NERNet) and number of epochs to train on. The loss is always CrossEntropyLoss and the optimizer is always Adam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avkHfjT3k0HM"
      },
      "source": [
        "def train_loop(model, n_epochs):\n",
        "  # Loss function\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Optimizer (ADAM is a fancy version of SGD)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "  \n",
        "  for e in range(1, n_epochs + 1):\n",
        "    for seq in train_sequences:\n",
        "      sentence, tags = seq\n",
        "      sentence_tensor = torch.LongTensor(sentence).cuda()\n",
        "      tags_tensor = torch.LongTensor(tags).cuda()\n",
        "\n",
        "      if len(sentence_tensor) == 0:\n",
        "        continue\n",
        "      \n",
        "      model.zero_grad()\n",
        "      scores = model.forward(sentence_tensor)\n",
        "      criterion(scores, tags_tensor).backward()\n",
        "      optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baN1c_B7lTjb"
      },
      "source": [
        "**Task 5:** write an evaluation loop on a trained model, using the dev and test datasets. This function print the true positive rate (TPR), also known as Recall and the opposite to false positive rate (FPR), also known as precision, of each label seperately (7 labels in total), and for all the 6 labels (except O) together. The caption argument for the function should be served for printing, so that when you print include it as a prefix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyQAjGaqmd8U"
      },
      "source": [
        "def evaluate(model, caption):\n",
        "  labels = [\"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-ORG\", \"I-ORG\"]\n",
        "  datasets = [dev_sequences, test_sequences]\n",
        "  print(\"tested model is: \" + caption + \"\\n\")\n",
        "\n",
        "  for data in datasets:\n",
        "    all_pred_labels = []\n",
        "    all_true_labels = []\n",
        "\n",
        "    for sentence, tags in data:\n",
        "      sentence_tensor = torch.LongTensor(sentence).cuda()\n",
        "      _, preds = model(sentence_tensor).T.max(0)\n",
        "      preds_list = preds.tolist()\n",
        "      all_pred_labels += preds_list\n",
        "      all_true_labels += tags\n",
        "\n",
        "      seperate_recall = recall_score(all_true_labels,\n",
        "                                     all_pred_labels,\n",
        "                                     average=None)\n",
        "      average_recall = recall_score(all_true_labels,\n",
        "                                    all_pred_labels,\n",
        "                                    labels=[1,2,3,4,5,6],\n",
        "                                    average='micro')\n",
        "\n",
        "      seperate_precision = precision_score(all_true_labels,\n",
        "                                           all_pred_labels,\n",
        "                                           average=None)\n",
        "      average_precision = precision_score(all_true_labels,\n",
        "                                          all_pred_labels,\n",
        "                                          labels=[1,2,3,4,5,6],\n",
        "                                          average='micro')\n",
        "\n",
        "    if data == dev_sequences:\n",
        "      print(\"Dev dataset results:\\n\")\n",
        "    else:\n",
        "      print(\"Test dataset results:\\n\")\n",
        "\n",
        "    print(\"Recall scores:\\n\")\n",
        "    for tag, score in zip(labels, seperate_recall):\n",
        "      print(\"tag: {}, Recall: {:.4f}\".format(tag, score))\n",
        "    print(\"\\nThe Recall result for all labels together (except O): {}\\n\".format(average_recall))\n",
        "\n",
        "    print(\"Precision scores:\\n\")\n",
        "    for tag, score in zip(labels, seperate_precision):\n",
        "      print(\"tag: {}, Precision: {:.4f}\".format(tag, score))\n",
        "    print(\"\\nThe Precision result for all labels together (except O): {}\".format(average_precision))\n",
        "\n",
        "    print(\"\\n------------------------------------------------------------------------------------\")\n",
        "    print(\"------------------------------------------------------------------------------------\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQSXqWNOmqG4"
      },
      "source": [
        "**Task 6:** Train and evaluate a few models, all with embedding_size=300, and with the following hyper parameters (you may use that as captions for the models as well):\n",
        "\n",
        "Model 1: (hidden_size: 500, n_layers: 1, directions: 1)\n",
        "\n",
        "Model 2: (hidden_size: 500, n_layers: 2, directions: 1)\n",
        "\n",
        "Model 3: (hidden_size: 500, n_layers: 3, directions: 1)\n",
        "\n",
        "Model 4: (hidden_size: 500, n_layers: 1, directions: 2)\n",
        "\n",
        "Model 5: (hidden_size: 500, n_layers: 2, directions: 2)\n",
        "\n",
        "Model 6: (hidden_size: 500, n_layers: 3, directions: 2)\n",
        "\n",
        "Model 4: (hidden_size: 800, n_layers: 1, directions: 2)\n",
        "\n",
        "Model 5: (hidden_size: 800, n_layers: 2, directions: 2)\n",
        "\n",
        "Model 6: (hidden_size: 800, n_layers: 3, directions: 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTNmBU6hycZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a4cd02-9585-4c5c-beb9-d5b2f6439d03"
      },
      "source": [
        "EMBEDDING_SIZE = 300\n",
        "EPOCHS = 10\n",
        "INPUT_SIZE = len(vocab.word2id)\n",
        "OUTPUT_SIZE = len(vocab.tag2id)\n",
        "\n",
        "models_config = [{'hidden_size': 500, 'layers':1, 'directions':1},\n",
        "                {'hidden_size': 500, 'layers':2, 'directions':1},\n",
        "                {'hidden_size': 500, 'layers':2, 'directions':1},\n",
        "                {'hidden_size': 500, 'layers':1, 'directions':2},\n",
        "                {'hidden_size': 500, 'layers':2, 'directions':2},\n",
        "                {'hidden_size': 500, 'layers':3, 'directions':2},\n",
        "                {'hidden_size': 800, 'layers':1, 'directions':2},\n",
        "                {'hidden_size': 800, 'layers':2, 'directions':2},\n",
        "                {'hidden_size': 800, 'layers':2, 'directions':2}]\n",
        "\n",
        "for i, config in enumerate(models_config, 1):\n",
        "    model_name = \"Model \" + str(i)\n",
        "    hidden_size = config['hidden_size']\n",
        "    n_layers = config['layers']\n",
        "    directions = config['directions']\n",
        "    model = NERNet(INPUT_SIZE, EMBEDDING_SIZE, hidden_size, OUTPUT_SIZE, n_layers, directions).cuda()\n",
        "    train_loop(model, EPOCHS)\n",
        "    evaluate(model, model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tested model is: Model 1\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9596\n",
            "tag: B-PER, Recall: 0.6700\n",
            "tag: I-PER, Recall: 0.6815\n",
            "tag: B-LOC, Recall: 0.7049\n",
            "tag: I-LOC, Recall: 0.4783\n",
            "tag: B-ORG, Recall: 0.5893\n",
            "tag: I-ORG, Recall: 0.4224\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6245572609208973\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9273\n",
            "tag: B-PER, Precision: 0.6802\n",
            "tag: I-PER, Precision: 0.7985\n",
            "tag: B-LOC, Precision: 0.7544\n",
            "tag: I-LOC, Precision: 0.7857\n",
            "tag: B-ORG, Precision: 0.6346\n",
            "tag: I-ORG, Precision: 0.7313\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7158322056833559\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9603\n",
            "tag: B-PER, Recall: 0.6705\n",
            "tag: I-PER, Recall: 0.6757\n",
            "tag: B-LOC, Recall: 0.6939\n",
            "tag: I-LOC, Recall: 0.5094\n",
            "tag: B-ORG, Recall: 0.5886\n",
            "tag: I-ORG, Recall: 0.3850\n",
            "\n",
            "The Recall result for all labels together (except O): 0.619928400954654\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9298\n",
            "tag: B-PER, Precision: 0.7203\n",
            "tag: I-PER, Precision: 0.7937\n",
            "tag: B-LOC, Precision: 0.7604\n",
            "tag: I-LOC, Precision: 0.9310\n",
            "tag: B-ORG, Precision: 0.6149\n",
            "tag: I-ORG, Precision: 0.6016\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7111567419575633\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 2\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9629\n",
            "tag: B-PER, Recall: 0.7150\n",
            "tag: I-PER, Recall: 0.7516\n",
            "tag: B-LOC, Recall: 0.7213\n",
            "tag: I-LOC, Recall: 0.4783\n",
            "tag: B-ORG, Recall: 0.5774\n",
            "tag: I-ORG, Recall: 0.3707\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6422668240850059\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9357\n",
            "tag: B-PER, Precision: 0.6530\n",
            "tag: I-PER, Precision: 0.7468\n",
            "tag: B-LOC, Precision: 0.8462\n",
            "tag: I-LOC, Precision: 0.6875\n",
            "tag: B-ORG, Precision: 0.6554\n",
            "tag: I-ORG, Precision: 0.7167\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7186261558784677\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9610\n",
            "tag: B-PER, Recall: 0.7120\n",
            "tag: I-PER, Recall: 0.7466\n",
            "tag: B-LOC, Recall: 0.7143\n",
            "tag: I-LOC, Recall: 0.6226\n",
            "tag: B-ORG, Recall: 0.6343\n",
            "tag: I-ORG, Recall: 0.3500\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6563245823389021\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9421\n",
            "tag: B-PER, Precision: 0.7055\n",
            "tag: I-PER, Precision: 0.7038\n",
            "tag: B-LOC, Precision: 0.8596\n",
            "tag: I-LOC, Precision: 0.7857\n",
            "tag: B-ORG, Precision: 0.6307\n",
            "tag: I-ORG, Precision: 0.6195\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7124352331606217\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 3\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9648\n",
            "tag: B-PER, Recall: 0.7000\n",
            "tag: I-PER, Recall: 0.6752\n",
            "tag: B-LOC, Recall: 0.6831\n",
            "tag: I-LOC, Recall: 0.3478\n",
            "tag: B-ORG, Recall: 0.6250\n",
            "tag: I-ORG, Recall: 0.4483\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6328217237308147\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9285\n",
            "tag: B-PER, Precision: 0.7609\n",
            "tag: I-PER, Precision: 0.8480\n",
            "tag: B-LOC, Precision: 0.8446\n",
            "tag: I-LOC, Precision: 0.8889\n",
            "tag: B-ORG, Precision: 0.5676\n",
            "tag: I-ORG, Precision: 0.6933\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7382920110192838\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9595\n",
            "tag: B-PER, Recall: 0.7005\n",
            "tag: I-PER, Recall: 0.7061\n",
            "tag: B-LOC, Recall: 0.6851\n",
            "tag: I-LOC, Recall: 0.4717\n",
            "tag: B-ORG, Recall: 0.6000\n",
            "tag: I-ORG, Recall: 0.3800\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6318615751789977\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9363\n",
            "tag: B-PER, Precision: 0.7600\n",
            "tag: I-PER, Precision: 0.8327\n",
            "tag: B-LOC, Precision: 0.8484\n",
            "tag: I-LOC, Precision: 0.9615\n",
            "tag: B-ORG, Precision: 0.5097\n",
            "tag: I-ORG, Precision: 0.5170\n",
            "\n",
            "The Precision result for all labels together (except O): 0.6999339061467283\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 4\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9651\n",
            "tag: B-PER, Recall: 0.6850\n",
            "tag: I-PER, Recall: 0.7452\n",
            "tag: B-LOC, Recall: 0.7760\n",
            "tag: I-LOC, Recall: 0.6087\n",
            "tag: B-ORG, Recall: 0.6071\n",
            "tag: I-ORG, Recall: 0.4224\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6623376623376623\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9343\n",
            "tag: B-PER, Precision: 0.7740\n",
            "tag: I-PER, Precision: 0.8417\n",
            "tag: B-LOC, Precision: 0.6698\n",
            "tag: I-LOC, Precision: 0.6667\n",
            "tag: B-ORG, Precision: 0.7286\n",
            "tag: I-ORG, Precision: 0.8750\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7530201342281879\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9711\n",
            "tag: B-PER, Recall: 0.6797\n",
            "tag: I-PER, Recall: 0.7095\n",
            "tag: B-LOC, Recall: 0.8280\n",
            "tag: I-LOC, Recall: 0.6226\n",
            "tag: B-ORG, Recall: 0.5686\n",
            "tag: I-ORG, Recall: 0.3400\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6497613365155132\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9361\n",
            "tag: B-PER, Precision: 0.8038\n",
            "tag: I-PER, Precision: 0.8235\n",
            "tag: B-LOC, Precision: 0.6860\n",
            "tag: I-LOC, Precision: 0.7021\n",
            "tag: B-ORG, Precision: 0.7960\n",
            "tag: I-ORG, Precision: 0.6939\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7610062893081762\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 5\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9780\n",
            "tag: B-PER, Recall: 0.6150\n",
            "tag: I-PER, Recall: 0.6752\n",
            "tag: B-LOC, Recall: 0.7705\n",
            "tag: I-LOC, Recall: 0.5217\n",
            "tag: B-ORG, Recall: 0.7202\n",
            "tag: I-ORG, Recall: 0.4914\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6611570247933884\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9366\n",
            "tag: B-PER, Precision: 0.8786\n",
            "tag: I-PER, Precision: 0.9381\n",
            "tag: B-LOC, Precision: 0.8294\n",
            "tag: I-LOC, Precision: 0.8000\n",
            "tag: B-ORG, Precision: 0.6111\n",
            "tag: I-ORG, Precision: 0.7703\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7887323943661971\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9779\n",
            "tag: B-PER, Recall: 0.6267\n",
            "tag: I-PER, Recall: 0.6993\n",
            "tag: B-LOC, Recall: 0.7872\n",
            "tag: I-LOC, Recall: 0.5472\n",
            "tag: B-ORG, Recall: 0.6943\n",
            "tag: I-ORG, Recall: 0.4300\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6605011933174224\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9410\n",
            "tag: B-PER, Precision: 0.9189\n",
            "tag: I-PER, Precision: 0.8734\n",
            "tag: B-LOC, Precision: 0.8060\n",
            "tag: I-LOC, Precision: 0.7838\n",
            "tag: B-ORG, Precision: 0.6361\n",
            "tag: I-ORG, Precision: 0.6565\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7806770098730607\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 6\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9725\n",
            "tag: B-PER, Recall: 0.7600\n",
            "tag: I-PER, Recall: 0.7898\n",
            "tag: B-LOC, Recall: 0.7705\n",
            "tag: I-LOC, Recall: 0.3913\n",
            "tag: B-ORG, Recall: 0.6190\n",
            "tag: I-ORG, Recall: 0.4483\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6871310507674144\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9486\n",
            "tag: B-PER, Precision: 0.8042\n",
            "tag: I-PER, Precision: 0.8322\n",
            "tag: B-LOC, Precision: 0.7747\n",
            "tag: I-LOC, Precision: 0.5625\n",
            "tag: B-ORG, Precision: 0.6887\n",
            "tag: I-ORG, Precision: 0.6341\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7568270481144344\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9708\n",
            "tag: B-PER, Recall: 0.7350\n",
            "tag: I-PER, Recall: 0.7669\n",
            "tag: B-LOC, Recall: 0.7522\n",
            "tag: I-LOC, Recall: 0.6038\n",
            "tag: B-ORG, Recall: 0.6429\n",
            "tag: I-ORG, Recall: 0.5100\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6939140811455847\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9464\n",
            "tag: B-PER, Precision: 0.8351\n",
            "tag: I-PER, Precision: 0.8376\n",
            "tag: B-LOC, Precision: 0.7522\n",
            "tag: I-LOC, Precision: 0.7805\n",
            "tag: B-ORG, Precision: 0.7401\n",
            "tag: I-ORG, Precision: 0.6145\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7717319177173192\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 7\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9832\n",
            "tag: B-PER, Recall: 0.6950\n",
            "tag: I-PER, Recall: 0.7070\n",
            "tag: B-LOC, Recall: 0.7104\n",
            "tag: I-LOC, Recall: 0.3913\n",
            "tag: B-ORG, Recall: 0.5774\n",
            "tag: I-ORG, Recall: 0.4138\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6304604486422668\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9266\n",
            "tag: B-PER, Precision: 0.8274\n",
            "tag: I-PER, Precision: 0.8880\n",
            "tag: B-LOC, Precision: 0.8387\n",
            "tag: I-LOC, Precision: 0.7500\n",
            "tag: B-ORG, Precision: 0.7405\n",
            "tag: I-ORG, Precision: 0.7164\n",
            "\n",
            "The Precision result for all labels together (except O): 0.8115501519756839\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9788\n",
            "tag: B-PER, Recall: 0.6751\n",
            "tag: I-PER, Recall: 0.7432\n",
            "tag: B-LOC, Recall: 0.7085\n",
            "tag: I-LOC, Recall: 0.5283\n",
            "tag: B-ORG, Recall: 0.5171\n",
            "tag: I-ORG, Recall: 0.3750\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6205250596658711\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9246\n",
            "tag: B-PER, Precision: 0.8116\n",
            "tag: I-PER, Precision: 0.8800\n",
            "tag: B-LOC, Precision: 0.8073\n",
            "tag: I-LOC, Precision: 0.9655\n",
            "tag: B-ORG, Precision: 0.7328\n",
            "tag: I-ORG, Precision: 0.7282\n",
            "\n",
            "The Precision result for all labels together (except O): 0.8055770720371804\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 8\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9797\n",
            "tag: B-PER, Recall: 0.7200\n",
            "tag: I-PER, Recall: 0.7325\n",
            "tag: B-LOC, Recall: 0.8033\n",
            "tag: I-LOC, Recall: 0.4348\n",
            "tag: B-ORG, Recall: 0.5774\n",
            "tag: I-ORG, Recall: 0.4224\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6635182998819362\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9344\n",
            "tag: B-PER, Precision: 0.8045\n",
            "tag: I-PER, Precision: 0.8712\n",
            "tag: B-LOC, Precision: 0.7737\n",
            "tag: I-LOC, Precision: 0.7143\n",
            "tag: B-ORG, Precision: 0.7823\n",
            "tag: I-ORG, Precision: 0.8448\n",
            "\n",
            "The Precision result for all labels together (except O): 0.806312769010043\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9772\n",
            "tag: B-PER, Recall: 0.7235\n",
            "tag: I-PER, Recall: 0.7770\n",
            "tag: B-LOC, Recall: 0.8047\n",
            "tag: I-LOC, Recall: 0.6226\n",
            "tag: B-ORG, Recall: 0.6000\n",
            "tag: I-ORG, Recall: 0.4650\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6897374701670644\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9413\n",
            "tag: B-PER, Precision: 0.8418\n",
            "tag: I-PER, Precision: 0.8812\n",
            "tag: B-LOC, Precision: 0.7500\n",
            "tag: I-LOC, Precision: 0.9167\n",
            "tag: B-ORG, Precision: 0.7749\n",
            "tag: I-ORG, Precision: 0.7949\n",
            "\n",
            "The Precision result for all labels together (except O): 0.8106591865357644\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 9\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9784\n",
            "tag: B-PER, Recall: 0.7250\n",
            "tag: I-PER, Recall: 0.7580\n",
            "tag: B-LOC, Recall: 0.7432\n",
            "tag: I-LOC, Recall: 0.4783\n",
            "tag: B-ORG, Recall: 0.6310\n",
            "tag: I-ORG, Recall: 0.4828\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6765053128689492\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9320\n",
            "tag: B-PER, Precision: 0.7754\n",
            "tag: I-PER, Precision: 0.8750\n",
            "tag: B-LOC, Precision: 0.8242\n",
            "tag: I-LOC, Precision: 1.0000\n",
            "tag: B-ORG, Precision: 0.8480\n",
            "tag: I-ORG, Precision: 0.8116\n",
            "\n",
            "The Precision result for all labels together (except O): 0.8268398268398268\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9767\n",
            "tag: B-PER, Recall: 0.7304\n",
            "tag: I-PER, Recall: 0.7736\n",
            "tag: B-LOC, Recall: 0.7609\n",
            "tag: I-LOC, Recall: 0.6038\n",
            "tag: B-ORG, Recall: 0.5657\n",
            "tag: I-ORG, Recall: 0.4250\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6694510739856802\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9374\n",
            "tag: B-PER, Precision: 0.7847\n",
            "tag: I-PER, Precision: 0.7869\n",
            "tag: B-LOC, Precision: 0.8208\n",
            "tag: I-LOC, Precision: 0.8889\n",
            "tag: B-ORG, Precision: 0.8285\n",
            "tag: I-ORG, Precision: 0.7522\n",
            "\n",
            "The Precision result for all labels together (except O): 0.8008565310492506\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM74r0_8nk5s"
      },
      "source": [
        "**Task 6:** Download the GloVe embeddings from https://nlp.stanford.edu/projects/glove/ (use the 300-dim vectors from glove.6B.zip). Then intialize the nn.Embedding module in your NERNet with these embeddings, so that you can start your training with pre-trained vectors. Repeat Task 6 and print the results for each model.\n",
        "\n",
        "Note: make sure that vectors are aligned with the IDs in your Vocab, in other words, make sure that for example the word with ID 0 is the first vector in the GloVe matrix of vectors that you initialize nn.Embedding with. For a dicussion on how to do that, check it this link:\n",
        "https://discuss.pytorch.org/t/can-we-use-pre-trained-word-embeddings-for-weight-initialization-in-nn-embedding/1222"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRiMbvx9o5Rh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89508577-7e71-4395-dcdc-b826868c3c11"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "GLOVE_PATH = 'glove.6B.300d.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-27 08:34:42--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-05-27 08:34:42--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-05-27 08:34:43--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.19MB/s    in 2m 40s  \n",
            "\n",
            "2022-05-27 08:37:23 (5.14 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings(path, word2id=vocab.word2id, dimension=300):\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((len(word2id), dimension))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2id.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()"
      ],
      "metadata": {
        "id": "B-VRSYDJycS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = load_embeddings(GLOVE_PATH)"
      ],
      "metadata": {
        "id": "DBcv2pRoyez2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, config in enumerate(models_config, 1):\n",
        "    model_name = \"Model \" + str(i)\n",
        "    hidden_size = config['hidden_size']\n",
        "    n_layers = config['layers']\n",
        "    directions = config['directions']\n",
        "    model_glove = NERNet(INPUT_SIZE, EMBEDDING_SIZE, hidden_size, OUTPUT_SIZE, n_layers, directions).cuda()\n",
        "    model_glove.embedding = nn.Embedding.from_pretrained(weights,freeze=True).cuda()\n",
        "    train_loop(model_glove, EPOCHS)\n",
        "    evaluate(model_glove, model_name)"
      ],
      "metadata": {
        "id": "CQBIvwIpyglf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5074e3c-4c4e-4d13-d9d8-0e622084ad3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tested model is: Model 1\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9474\n",
            "tag: B-PER, Recall: 0.5450\n",
            "tag: I-PER, Recall: 0.7006\n",
            "tag: B-LOC, Recall: 0.6393\n",
            "tag: I-LOC, Recall: 0.3913\n",
            "tag: B-ORG, Recall: 0.6845\n",
            "tag: I-ORG, Recall: 0.6034\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6257378984651711\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9566\n",
            "tag: B-PER, Precision: 0.7956\n",
            "tag: I-PER, Precision: 0.8209\n",
            "tag: B-LOC, Precision: 0.7748\n",
            "tag: I-LOC, Precision: 0.7500\n",
            "tag: B-ORG, Precision: 0.4228\n",
            "tag: I-ORG, Precision: 0.4094\n",
            "\n",
            "The Precision result for all labels together (except O): 0.6043329532497149\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9493\n",
            "tag: B-PER, Recall: 0.4885\n",
            "tag: I-PER, Recall: 0.6486\n",
            "tag: B-LOC, Recall: 0.6006\n",
            "tag: I-LOC, Recall: 0.4528\n",
            "tag: B-ORG, Recall: 0.6886\n",
            "tag: I-ORG, Recall: 0.6550\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6002386634844868\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9595\n",
            "tag: B-PER, Precision: 0.8092\n",
            "tag: I-PER, Precision: 0.7773\n",
            "tag: B-LOC, Precision: 0.6890\n",
            "tag: I-LOC, Precision: 0.5581\n",
            "tag: B-ORG, Precision: 0.4199\n",
            "tag: I-ORG, Precision: 0.4081\n",
            "\n",
            "The Precision result for all labels together (except O): 0.5761741122565864\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 2\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9490\n",
            "tag: B-PER, Recall: 0.5100\n",
            "tag: I-PER, Recall: 0.6752\n",
            "tag: B-LOC, Recall: 0.6393\n",
            "tag: I-LOC, Recall: 0.3913\n",
            "tag: B-ORG, Recall: 0.7083\n",
            "tag: I-ORG, Recall: 0.5345\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6080283353010626\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9527\n",
            "tag: B-PER, Precision: 0.8293\n",
            "tag: I-PER, Precision: 0.8217\n",
            "tag: B-LOC, Precision: 0.7959\n",
            "tag: I-LOC, Precision: 0.6923\n",
            "tag: B-ORG, Precision: 0.4146\n",
            "tag: I-ORG, Precision: 0.3875\n",
            "\n",
            "The Precision result for all labels together (except O): 0.59953434225844\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9514\n",
            "tag: B-PER, Recall: 0.4240\n",
            "tag: I-PER, Recall: 0.6284\n",
            "tag: B-LOC, Recall: 0.6152\n",
            "tag: I-LOC, Recall: 0.3774\n",
            "tag: B-ORG, Recall: 0.7000\n",
            "tag: I-ORG, Recall: 0.6250\n",
            "\n",
            "The Recall result for all labels together (except O): 0.5793556085918854\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9554\n",
            "tag: B-PER, Precision: 0.8106\n",
            "tag: I-PER, Precision: 0.7718\n",
            "tag: B-LOC, Precision: 0.6720\n",
            "tag: I-LOC, Precision: 0.6061\n",
            "tag: B-ORG, Precision: 0.4160\n",
            "tag: I-ORG, Precision: 0.4181\n",
            "\n",
            "The Precision result for all labels together (except O): 0.5701702877275396\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 3\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9496\n",
            "tag: B-PER, Recall: 0.5400\n",
            "tag: I-PER, Recall: 0.6688\n",
            "tag: B-LOC, Recall: 0.6230\n",
            "tag: I-LOC, Recall: 0.3043\n",
            "tag: B-ORG, Recall: 0.6667\n",
            "tag: I-ORG, Recall: 0.6207\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6115702479338843\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9577\n",
            "tag: B-PER, Precision: 0.7826\n",
            "tag: I-PER, Precision: 0.8077\n",
            "tag: B-LOC, Precision: 0.7972\n",
            "tag: I-LOC, Precision: 0.5833\n",
            "tag: B-ORG, Precision: 0.4058\n",
            "tag: I-ORG, Precision: 0.4138\n",
            "\n",
            "The Precision result for all labels together (except O): 0.5933562428407789\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9504\n",
            "tag: B-PER, Recall: 0.4631\n",
            "tag: I-PER, Recall: 0.6520\n",
            "tag: B-LOC, Recall: 0.6006\n",
            "tag: I-LOC, Recall: 0.3962\n",
            "tag: B-ORG, Recall: 0.7114\n",
            "tag: I-ORG, Recall: 0.6800\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6002386634844868\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9604\n",
            "tag: B-PER, Precision: 0.7976\n",
            "tag: I-PER, Precision: 0.7689\n",
            "tag: B-LOC, Precision: 0.7007\n",
            "tag: I-LOC, Precision: 0.5676\n",
            "tag: B-ORG, Precision: 0.4256\n",
            "tag: I-ORG, Precision: 0.4172\n",
            "\n",
            "The Precision result for all labels together (except O): 0.5765042979942694\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 4\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9742\n",
            "tag: B-PER, Recall: 0.7350\n",
            "tag: I-PER, Recall: 0.8471\n",
            "tag: B-LOC, Recall: 0.7486\n",
            "tag: I-LOC, Recall: 0.4783\n",
            "tag: B-ORG, Recall: 0.5655\n",
            "tag: I-ORG, Recall: 0.5948\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6989374262101535\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9572\n",
            "tag: B-PER, Precision: 0.7656\n",
            "tag: I-PER, Precision: 0.8581\n",
            "tag: B-LOC, Precision: 0.7098\n",
            "tag: I-LOC, Precision: 0.5238\n",
            "tag: B-ORG, Precision: 0.6934\n",
            "tag: I-ORG, Precision: 0.7340\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7474747474747475\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9703\n",
            "tag: B-PER, Recall: 0.6866\n",
            "tag: I-PER, Recall: 0.8142\n",
            "tag: B-LOC, Recall: 0.7522\n",
            "tag: I-LOC, Recall: 0.4717\n",
            "tag: B-ORG, Recall: 0.5914\n",
            "tag: I-ORG, Recall: 0.5650\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6813842482100239\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9532\n",
            "tag: B-PER, Precision: 0.7905\n",
            "tag: I-PER, Precision: 0.8486\n",
            "tag: B-LOC, Precision: 0.6582\n",
            "tag: I-LOC, Precision: 0.5556\n",
            "tag: B-ORG, Precision: 0.7289\n",
            "tag: I-ORG, Precision: 0.6420\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7329910141206675\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 5\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9742\n",
            "tag: B-PER, Recall: 0.7450\n",
            "tag: I-PER, Recall: 0.8599\n",
            "tag: B-LOC, Recall: 0.7760\n",
            "tag: I-LOC, Recall: 0.5217\n",
            "tag: B-ORG, Recall: 0.6310\n",
            "tag: I-ORG, Recall: 0.5948\n",
            "\n",
            "The Recall result for all labels together (except O): 0.7237308146399055\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9608\n",
            "tag: B-PER, Precision: 0.8278\n",
            "tag: I-PER, Precision: 0.9000\n",
            "tag: B-LOC, Precision: 0.7802\n",
            "tag: I-LOC, Precision: 0.8000\n",
            "tag: B-ORG, Precision: 0.6424\n",
            "tag: I-ORG, Precision: 0.6161\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7624378109452736\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9784\n",
            "tag: B-PER, Recall: 0.6774\n",
            "tag: I-PER, Recall: 0.8074\n",
            "tag: B-LOC, Recall: 0.7464\n",
            "tag: I-LOC, Recall: 0.6604\n",
            "tag: B-ORG, Recall: 0.6714\n",
            "tag: I-ORG, Recall: 0.6600\n",
            "\n",
            "The Recall result for all labels together (except O): 0.7106205250596659\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9584\n",
            "tag: B-PER, Precision: 0.8448\n",
            "tag: I-PER, Precision: 0.9157\n",
            "tag: B-LOC, Precision: 0.7314\n",
            "tag: I-LOC, Precision: 0.6250\n",
            "tag: B-ORG, Precision: 0.7532\n",
            "tag: I-ORG, Precision: 0.6226\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7738791423001949\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 6\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9745\n",
            "tag: B-PER, Recall: 0.7850\n",
            "tag: I-PER, Recall: 0.8408\n",
            "tag: B-LOC, Recall: 0.7158\n",
            "tag: I-LOC, Recall: 0.3913\n",
            "tag: B-ORG, Recall: 0.6369\n",
            "tag: I-ORG, Recall: 0.6293\n",
            "\n",
            "The Recall result for all labels together (except O): 0.71900826446281\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9602\n",
            "tag: B-PER, Precision: 0.8351\n",
            "tag: I-PER, Precision: 0.8859\n",
            "tag: B-LOC, Precision: 0.8037\n",
            "tag: I-LOC, Precision: 0.6923\n",
            "tag: B-ORG, Precision: 0.6446\n",
            "tag: I-ORG, Precision: 0.5984\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7602996254681648\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9782\n",
            "tag: B-PER, Recall: 0.6843\n",
            "tag: I-PER, Recall: 0.8176\n",
            "tag: B-LOC, Recall: 0.7085\n",
            "tag: I-LOC, Recall: 0.5660\n",
            "tag: B-ORG, Recall: 0.6743\n",
            "tag: I-ORG, Recall: 0.7000\n",
            "\n",
            "The Recall result for all labels together (except O): 0.7088305489260143\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9571\n",
            "tag: B-PER, Precision: 0.8462\n",
            "tag: I-PER, Precision: 0.8963\n",
            "tag: B-LOC, Precision: 0.7666\n",
            "tag: I-LOC, Precision: 0.7692\n",
            "tag: B-ORG, Precision: 0.7217\n",
            "tag: I-ORG, Precision: 0.6167\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7759634225996082\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 7\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9771\n",
            "tag: B-PER, Recall: 0.7550\n",
            "tag: I-PER, Recall: 0.8408\n",
            "tag: B-LOC, Recall: 0.6885\n",
            "tag: I-LOC, Recall: 0.4783\n",
            "tag: B-ORG, Recall: 0.5833\n",
            "tag: I-ORG, Recall: 0.5172\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6824085005903188\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9501\n",
            "tag: B-PER, Precision: 0.7704\n",
            "tag: I-PER, Precision: 0.8859\n",
            "tag: B-LOC, Precision: 0.7545\n",
            "tag: I-LOC, Precision: 0.5500\n",
            "tag: B-ORG, Precision: 0.6950\n",
            "tag: I-ORG, Precision: 0.6977\n",
            "\n",
            "The Precision result for all labels together (except O): 0.761528326745718\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9765\n",
            "tag: B-PER, Recall: 0.6774\n",
            "tag: I-PER, Recall: 0.7804\n",
            "tag: B-LOC, Recall: 0.6793\n",
            "tag: I-LOC, Recall: 0.3962\n",
            "tag: B-ORG, Recall: 0.5857\n",
            "tag: I-ORG, Recall: 0.5350\n",
            "\n",
            "The Recall result for all labels together (except O): 0.6509546539379475\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9425\n",
            "tag: B-PER, Precision: 0.8235\n",
            "tag: I-PER, Precision: 0.8556\n",
            "tag: B-LOC, Precision: 0.7039\n",
            "tag: I-LOC, Precision: 0.5250\n",
            "tag: B-ORG, Precision: 0.7482\n",
            "tag: I-ORG, Precision: 0.6407\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7581653926337735\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 8\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9761\n",
            "tag: B-PER, Recall: 0.7750\n",
            "tag: I-PER, Recall: 0.8408\n",
            "tag: B-LOC, Recall: 0.7322\n",
            "tag: I-LOC, Recall: 0.4348\n",
            "tag: B-ORG, Recall: 0.6667\n",
            "tag: I-ORG, Recall: 0.6207\n",
            "\n",
            "The Recall result for all labels together (except O): 0.7260920897284534\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9612\n",
            "tag: B-PER, Precision: 0.8245\n",
            "tag: I-PER, Precision: 0.8800\n",
            "tag: B-LOC, Precision: 0.7976\n",
            "tag: I-LOC, Precision: 0.7692\n",
            "tag: B-ORG, Precision: 0.6627\n",
            "tag: I-ORG, Precision: 0.6486\n",
            "\n",
            "The Precision result for all labels together (except O): 0.769712140175219\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9785\n",
            "tag: B-PER, Recall: 0.7166\n",
            "tag: I-PER, Recall: 0.8547\n",
            "tag: B-LOC, Recall: 0.7405\n",
            "tag: I-LOC, Recall: 0.6226\n",
            "tag: B-ORG, Recall: 0.7200\n",
            "tag: I-ORG, Recall: 0.6650\n",
            "\n",
            "The Recall result for all labels together (except O): 0.7374701670644391\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9636\n",
            "tag: B-PER, Precision: 0.8497\n",
            "tag: I-PER, Precision: 0.8694\n",
            "tag: B-LOC, Precision: 0.7815\n",
            "tag: I-LOC, Precision: 0.6346\n",
            "tag: B-ORG, Precision: 0.7221\n",
            "tag: I-ORG, Precision: 0.6963\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7852604828462516\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "tested model is: Model 9\n",
            "\n",
            "Dev dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9738\n",
            "tag: B-PER, Recall: 0.7800\n",
            "tag: I-PER, Recall: 0.8344\n",
            "tag: B-LOC, Recall: 0.6885\n",
            "tag: I-LOC, Recall: 0.3478\n",
            "tag: B-ORG, Recall: 0.6310\n",
            "tag: I-ORG, Recall: 0.6293\n",
            "\n",
            "The Recall result for all labels together (except O): 0.7083825265643447\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9602\n",
            "tag: B-PER, Precision: 0.8168\n",
            "tag: I-PER, Precision: 0.8733\n",
            "tag: B-LOC, Precision: 0.8182\n",
            "tag: I-LOC, Precision: 0.7273\n",
            "tag: B-ORG, Precision: 0.6310\n",
            "tag: I-ORG, Precision: 0.5659\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7471980074719801\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "Test dataset results:\n",
            "\n",
            "Recall scores:\n",
            "\n",
            "tag: O, Recall: 0.9729\n",
            "tag: B-PER, Recall: 0.6982\n",
            "tag: I-PER, Recall: 0.8446\n",
            "tag: B-LOC, Recall: 0.6735\n",
            "tag: I-LOC, Recall: 0.5283\n",
            "tag: B-ORG, Recall: 0.7029\n",
            "tag: I-ORG, Recall: 0.7400\n",
            "\n",
            "The Recall result for all labels together (except O): 0.7195704057279236\n",
            "\n",
            "Precision scores:\n",
            "\n",
            "tag: O, Precision: 0.9593\n",
            "tag: B-PER, Precision: 0.8417\n",
            "tag: I-PER, Precision: 0.8834\n",
            "tag: B-LOC, Precision: 0.8049\n",
            "tag: I-LOC, Precision: 0.6222\n",
            "tag: B-ORG, Precision: 0.6949\n",
            "tag: I-ORG, Precision: 0.5827\n",
            "\n",
            "The Precision result for all labels together (except O): 0.7618445988629186\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxaESRoco6bV"
      },
      "source": [
        "**Good luck!**"
      ]
    }
  ]
}